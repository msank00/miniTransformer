{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3c823b1-306b-4906-be34-8cb8259e044b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dbc4ad7-9e99-4007-8a75-10dc82eb22a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e4ec047-ee9b-49f8-9bb4-7c722b61b2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064361cf-bfba-43bf-a8bb-3eaff80c015e",
   "metadata": {},
   "source": [
    "# Embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ece1155a-7757-4dbb-821c-f84501a19bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder(nn.Module):\n",
    "    def __init__(self, vocab_size:int, emb_dim:int):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim \n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim)\n",
    "\n",
    "    def forward(self, x: torch.tensor):\n",
    "        return self.embed(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1f847a-d0c4-4944-8898-9ea00c9fe419",
   "metadata": {},
   "source": [
    "# Why use `register_buffer`\n",
    "\n",
    "- Ans [link](https://discuss.pytorch.org/t/what-is-the-difference-between-register-buffer-and-register-parameter-of-nn-module/32723/11)\n",
    "\n",
    "An example where I find this distinction difficult is in \n",
    "the context of fixed positional encodings in the Transformer \n",
    "model. Typically I see implementations where the fixed positional \n",
    "encodings are registered as buffers but I’d consider these tensors \n",
    "as non-learnable parameters (that should show up in the list of \n",
    "model parameters), especially when comparing between methods \n",
    "that don’t rely on such injection of fixed tensors.\n",
    "\n",
    "So in general:\n",
    "- buffers = `fixed tensors / non-learnable parameters / stuff that does not require gradient`\n",
    "- parameters = `learnable parameters, requires gradient`\n",
    "\n",
    "![image](https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/ptrblck/90/1823_2.png)\n",
    "Piotr Bialecki\n",
    "\n",
    "If you have parameters in your model, which should be saved and restored in the state_dict, but not trained by the optimizer, you should register them as buffers. Buffers won’t be returned in model.parameters(), so that the optimizer won’t have a change to update them.\n",
    "\n",
    "Both approaches work the same regarding training etc.\n",
    "There are some differences in the function calls however. Using register_parameter you have to pass the name as a string, which can make the creation of a range of parameters convenient. Besides that I think it’s just coding style which one you prefer.\n",
    "\n",
    "If your `self.some_params` are `nn.Parameter` objects, then you don’t have to worry about this. If they’re tensors, then they won’t be in the `state_dict` (unless registered as buffer).\n",
    "\n",
    "> simple `torch.tensor` will not be available under `state_dict`\n",
    "\n",
    "one reason to register the tensor as a buffer is to be able to serialize the model and restore all internal states.\n",
    "Another one is that all buffers and parameters will be pushed to the device, if called on the parent model:\n",
    "\n",
    "```python\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.my_tensor = torch.randn(1)\n",
    "        self.register_buffer('my_buffer', torch.randn(1))\n",
    "        self.my_param = nn.Parameter(torch.randn(1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "            return x\n",
    "\n",
    "model = MyModel()\n",
    "print(model.my_tensor)\n",
    "> tensor([0.9329])\n",
    "print(model.state_dict())\n",
    "> OrderedDict([('my_param', tensor([-0.2471])), ('my_buffer', tensor([1.2112]))])\n",
    "\n",
    "model.cuda()\n",
    "print(model.my_tensor)\n",
    "> tensor([0.9329])\n",
    "print(model.state_dict())\n",
    "> OrderedDict([('my_param', tensor([-0.2471], device='cuda:0')), ('my_buffer', tensor([1.2112], device='cuda:0'))])\n",
    "```\n",
    "\n",
    "As you can see, model.my_tensor is still on the CPU, where is was created, while all parameters and buffers were pushed to the GPU after calling `model.cuda()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200974c3-2498-4e28-aa17-29a3d6826397",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"https://miro.medium.com/max/566/1*B-VR6R5vJl3Y7jbMNf5Fpw.png\" width=\"400\">\n",
    "</center>\n",
    "\n",
    "# Make embedding relatively larger by scaling the values. WHY?\n",
    "\n",
    "The reason we increase the embedding values before \n",
    "addition is to make the positional encoding relatively \n",
    "smaller. This means the original meaning in the embedding \n",
    "vector won’t be lost when we add them together\n",
    "\n",
    "```python\n",
    "x = x*math.sqrt(self.emb_dim)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31365422-2cda-43d6-947f-8554939ccad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, emb_dim:int, max_seq_len:int = 200, dropout_pct:float = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.dropout = nn.Dropout(dropout_pct)\n",
    "\n",
    "        # create constant 'pe' matrix with values dependent on \n",
    "        # word position 'pos' and embedding position 'i'\n",
    "        pe = torch.zeros(max_seq_len, emb_dim)\n",
    "\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, emb_dim, 2):\n",
    "                pe[pos,i] = math.sin(pos/(1000**((2*i)/emb_dim)))\n",
    "                pe[pos,i+1] = math.sin(pos/(1000**((2*(i+1))/emb_dim)))\n",
    "\n",
    "        # print(pe.size())\n",
    "        # pe = pe.unsqueeze(0)\n",
    "        # print(pe.size())\n",
    "\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # scale values\n",
    "        x = x*math.sqrt(self.emb_dim)  \n",
    "\n",
    "        seq_len = x.size(0)\n",
    "        \n",
    "        # add constant positional embedding to the word embedding\n",
    "        pe = Variable(self.pe[:seq_len,:], requires_grad=False)\n",
    "        \n",
    "        if x.is_cuda:\n",
    "            pe.cuda()\n",
    "        \n",
    "        x = x + pe\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379c04b9-bcdf-4eb2-8b0a-b89bc83332d7",
   "metadata": {},
   "source": [
    "## Test module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbab3d5a-002d-4347-b45c-081a2358b93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 20000\n",
    "EMB_DIM = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "498d138b-f422-4578-8fa3-4fa02e3034e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = Embedder(VOCAB_SIZE, EMB_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "767a1dea-4fce-4307-a93b-c0e016b78a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = torch.tensor([1,2,3,4,5]) # torch.randint(3, 5, (3,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b73df2a1-9e51-41e7-92e4-a25a78974a3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f09c443d-29ce-40ab-a2e4-f6243cb3ead9",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = e(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd525d96-7abf-48ba-abf1-6b03929a027a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 512])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "adbe5873-f596-47e2-9a6e-ff9e50f8420b",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = PositionalEmbedding(emb_dim=EMB_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a1e93fd-04ac-451c-9964-4e58b6491e03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 512])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.pe.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c62a48cb-004a-4e5e-ac9b-b82a5718752f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 512])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.pe.squeeze().size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca3827bb-906c-471e-8c8d-c01992587f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_emb = p(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b14d4fc-dea2-43b8-a957-d6989c999e10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 512])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_emb.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42cacf2-5830-42ff-9f92-3ba0e3c3d383",
   "metadata": {},
   "source": [
    "# Batch Normalization\n",
    "\n",
    "Normalisation is highly important in deep neural networks. It prevents the range of values in the layers changing too much, meaning the model trains faster and has better ability to generalise.\n",
    "\n",
    "<center>\n",
    "    <img src=\"https://miro.medium.com/max/511/1*4w3sQ14caDRkrQsAeK5Flw.png\" width=\"400\">\n",
    "</center>\n",
    "\n",
    "We will be normalising our results between each layer in the encoder/decoder, so before building our model let’s define that function:\n",
    "\n",
    "- [blog](https://kharshit.github.io/blog/2018/12/28/why-batch-normalization)\n",
    "\n",
    "<center>\n",
    "    <img src=\"https://kharshit.github.io/img/batch_normalization.png\" width=\"400\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "62b42188-7c89-4370-a146-019e8646eaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Norm(nn.Module):\n",
    "    def __init__(self, d_model, eps = 1e-6):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.size = d_model\n",
    "        \n",
    "        # create two learnable parameters to calibrate normalisation\n",
    "        self.alpha = nn.Parameter(torch.ones(self.size))\n",
    "        self.bias = nn.Parameter(torch.zeros(self.size))\n",
    "        \n",
    "        self.eps = eps\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x_mean = x.mean(dim=-1, keepdim=True)\n",
    "        x_variance = x.std(dim=-1, keepdim=True) \n",
    "        \n",
    "        normalized_x = (x - x_mean) / (x_variance + self.eps)\n",
    "        \n",
    "        # scale and shift\n",
    "        y = self.alpha * normalized_x + self.bias\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5f9182-0a0b-4b08-bef2-912ef6a41975",
   "metadata": {},
   "source": [
    "## Test module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "393b7769-337a-420b-b788-49633a359d3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.3549, 0.3643, 0.6217, 0.2935, 0.4385],\n",
       "         [0.5275, 0.4559, 0.7918, 0.7242, 0.2137],\n",
       "         [0.0652, 0.1690, 0.8499, 0.0375, 0.5418],\n",
       "         [0.4561, 0.7032, 0.9282, 0.8615, 0.6306],\n",
       "         [0.7029, 0.7938, 0.6353, 0.6353, 0.4864],\n",
       "         [0.0937, 0.5451, 0.6244, 0.8331, 0.3770],\n",
       "         [0.8098, 0.4133, 0.6112, 0.4413, 0.6418],\n",
       "         [0.3863, 0.5925, 0.6579, 0.5201, 0.8527],\n",
       "         [0.7962, 0.4085, 0.9211, 0.4699, 0.0184],\n",
       "         [0.3266, 0.6983, 0.2370, 0.2909, 0.4459]],\n",
       "\n",
       "        [[0.3323, 0.7005, 0.1673, 0.8067, 0.7122],\n",
       "         [0.2645, 0.6763, 0.9156, 0.9988, 0.9627],\n",
       "         [0.7283, 0.0362, 0.6782, 0.1660, 0.4566],\n",
       "         [0.0054, 0.6466, 0.1947, 0.4022, 0.3815],\n",
       "         [0.2907, 0.4536, 0.6271, 0.7440, 0.0601],\n",
       "         [0.1551, 0.7979, 0.3085, 0.1244, 0.5133],\n",
       "         [0.5799, 0.5476, 0.6216, 0.6387, 0.4013],\n",
       "         [0.2493, 0.1290, 0.2412, 0.1952, 0.3215],\n",
       "         [0.3790, 0.4681, 0.7214, 0.4209, 0.5561],\n",
       "         [0.1416, 0.3479, 0.8438, 0.3940, 0.9808]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_model = 5\n",
    "bs = 2\n",
    "seq_len = 10\n",
    "x = torch.rand(size=(bs,seq_len, d_model))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7e738aa-2165-4f8e-8b38-af80adb10ac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 5])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0a5bc488-5b0a-4261-84a7-faea341686c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = Norm(d_model=d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "00065cd0-bbc9-45bf-934d-07e5cf0c2461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4710, -0.3965,  1.6345, -0.9558,  0.1888],\n",
       "         [-0.0659, -0.3777,  1.0849,  0.7907, -1.4320],\n",
       "         [-0.7590, -0.4645,  1.4677, -0.8377,  0.5935],\n",
       "         [-1.3830, -0.0676,  1.1299,  0.7747, -0.4540],\n",
       "         [ 0.4633,  1.2713, -0.1372, -0.1374, -1.4599],\n",
       "         [-1.4436,  0.1816,  0.4671,  1.2187, -0.4238],\n",
       "         [ 1.3998, -1.0525,  0.1713, -0.8793,  0.3608],\n",
       "         [-1.2486, -0.0542,  0.3241, -0.4736,  1.4522],\n",
       "         [ 0.7704, -0.3223,  1.1224, -0.1490, -1.4215],\n",
       "         [-0.3982,  1.6253, -0.8862, -0.5924,  0.2515]],\n",
       "\n",
       "        [[-0.7616,  0.5641, -1.3556,  0.9467,  0.6064],\n",
       "         [-1.6304, -0.2853,  0.4965,  0.7686,  0.6506],\n",
       "         [ 1.0300, -1.2314,  0.8665, -0.8073,  0.1422],\n",
       "         [-1.3325,  1.3317, -0.5458,  0.3163,  0.2304],\n",
       "         [-0.5324,  0.0682,  0.7079,  1.1393, -1.3830],\n",
       "         [-0.8029,  1.4935, -0.2550, -0.9124,  0.4767],\n",
       "         [ 0.2332, -0.1080,  0.6754,  0.8562, -1.6567],\n",
       "         [ 0.3099, -1.3807,  0.1962, -0.4506,  1.3251],\n",
       "         [-0.9587, -0.3018,  1.5644, -0.6500,  0.3462],\n",
       "         [-1.1274, -0.5461,  0.8516, -0.4161,  1.2379]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81828f13-b3c5-4cea-9db3-3e00f1311722",
   "metadata": {},
   "source": [
    "# Attention\n",
    "\n",
    "<center>\n",
    "    <img src=\"https://miro.medium.com/max/445/1*evdACdTOBT5j1g1nXialBg.png\" width=\"400\">\n",
    "</center>\n",
    "\n",
    "\n",
    "<center>\n",
    "    <img src=\"https://miro.medium.com/max/140/1*15E9qKg9bKnWdSRWCyY2iA.png\" width=\"100\">\n",
    "</center>\n",
    "\n",
    "- Initially we must multiply $Q$ by the transpose of $K$. This is then `scaled` by dividing the output by the square root of $d_k$.\n",
    "- A step that’s not shown in the equation is the `masking operation`. Before we perform `Softmax`, we apply our mask and hence reduce values where the input is padding (or in the decoder, also where the input is ahead of the current word).\n",
    "\n",
    "Another step not shown is `dropout`, which we will apply after `Softmax`.\n",
    "\n",
    "Finally, the last step is doing a `dot` product between the result so far and $V$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c297cf51-86df-4ef0-bc85-c2e586c3407d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(q, k, v, d_k, mask=None, dropout=None):\n",
    "    \n",
    "    scores = torch.matmul(q, k.transpose(-2, -1)) /  math.sqrt(d_k)\n",
    "    \n",
    "    if mask is not None:\n",
    "        mask = mask.unsqueeze(1)\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    \n",
    "    scores = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    if dropout is not None:\n",
    "        scores = dropout(scores)\n",
    "        \n",
    "    output = torch.matmul(scores, v)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702ed8c6-1039-4511-b3ae-01fd7ff0f7fe",
   "metadata": {},
   "source": [
    "# Multi-Headed Attention\n",
    "\n",
    "Once we have our embedded values (with positional encodings) and our masks, we can start building the layers of our model.\n",
    "\n",
    "Here is an overview of the multi-headed attention layer:\n",
    "\n",
    "\n",
    "<center>\n",
    "    <img src=\"https://miro.medium.com/max/523/1*1tsRtfaY9z6HxmERYhw8XQ.png\" width=\"300\">\n",
    "</center>\n",
    "\n",
    "- $V$, $K$ and $Q$ stand for `key`, `value` and `query`. These are terms used in attention functions\n",
    "\n",
    "- In the case of the **Encoder**, $V, K$ and $G$ will simply be **identical copies** of the `emb_vector + pos_encoding`. \n",
    "- They will have the dimensions `Batch_size * seq_len * d_model`\n",
    "\n",
    "<center>\n",
    "    <img src=\"images/tensor_dimension.png\" width=\"400\">\n",
    "</center>\n",
    "\n",
    "- In multi-head attention we `split the embedding vector` into `N heads`. $d_{model}^{new} = \\frac{d_{model}}{N}$. so they will then have the dimensions `batch_size * N * seq_len * (d_model / N)`.\n",
    "- This final dimension `(d_model / N )` we will refer to as `d_k`\n",
    "\n",
    "- Drawing tool [Excalidraw](https://excalidraw.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5444bcb0-2cb0-482f-a4eb-91ab675e8c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads, d_model, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // heads\n",
    "        self.h = heads\n",
    "        \n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \n",
    "        bs = q.size(0)\n",
    "        \n",
    "        # perform linear operation and split into N heads\n",
    "        k = self.k_linear(k).view(bs, -1, self.h, self.d_k)\n",
    "        q = self.q_linear(q).view(bs, -1, self.h, self.d_k)\n",
    "        v = self.v_linear(v).view(bs, -1, self.h, self.d_k)\n",
    "        \n",
    "        # transpose to get dimensions bs * N * sl * d_model\n",
    "        k = k.transpose(1,2)\n",
    "        q = q.transpose(1,2)\n",
    "        v = v.transpose(1,2)\n",
    "        \n",
    "\n",
    "        # calculate attention using function we will define next\n",
    "        scores = attention(q, k, v, self.d_k, mask, self.dropout)\n",
    "        # concatenate heads and put through final linear layer\n",
    "        concat = scores.transpose(1,2).contiguous()\\\n",
    "        .view(bs, -1, self.d_model)\n",
    "        output = self.out(concat)\n",
    "    \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a430f667-cf3e-4207-80de-d644f67a634e",
   "metadata": {},
   "source": [
    "## Test module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9b59834e-fee0-443d-99f3-9625f71a3ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 16\n",
    "heads = 4\n",
    "bs = 2\n",
    "seq_len = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d3b481fe-f117-4aa4-85b7-98353efaca59",
   "metadata": {},
   "outputs": [],
   "source": [
    "mha = MultiHeadAttention(heads=heads, d_model=d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c2466916-af88-43e3-a26e-06d614bce4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = torch.rand(size=(bs,seq_len, d_model))\n",
    "k = torch.rand(size=(bs,seq_len, d_model))\n",
    "v = torch.rand(size=(bs,seq_len, d_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "210ea68b-8c18-4ffd-856d-175b794ff1eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 16])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = mha(q,k,v)\n",
    "output.size() # batch_size x sequence_length x model_dimension "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f40730c-7d08-4f0a-ae89-8b53b9c0e8fb",
   "metadata": {},
   "source": [
    "## Feedforward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e74032b5-48c0-44cb-801a-fff8cec8437c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model:int, d_ff:int = 2048, dropout_pct:float = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout_pct)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.dropout(F.relu(self.linear1(x)))\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c961993a-818b-469d-9b2c-6a5fdc05a387",
   "metadata": {},
   "source": [
    "## Is dropout applied before or after the non-linear activation function?\n",
    "\n",
    "Typically, dropout is applied after the non-linear activation function (a). However, when using rectified linear units (ReLUs), it might make sense to apply dropout before the non-linear activation (b) for reasons of computational efficiency depending on the particular code implementation.\n",
    "\n",
    "- (a): Fully connected, `linear activation` -> `ReLU` -> `Dropout` -> … [traditional use]\n",
    "- (b): Fully connected, `linear activation` -> `Dropout` -> `ReLU` -> …\n",
    "\n",
    "**Reference:**\n",
    "- [Sebastian Rachka](https://sebastianraschka.com/faq/docs/dropout-activation.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f358ae97-79bb-4322-bdc6-bfdf385f99c5",
   "metadata": {},
   "source": [
    "## Test module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0af51322-44c0-48d1-a1fc-778716be5c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 16\n",
    "bs = 2\n",
    "seq_len = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b3fc7071-ab1c-464b-8b49-36892fda2aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(size=(bs,seq_len, d_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "14d67507-92c6-48a4-973c-72529e7ca760",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff = FeedForward(d_model=d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "27b852c8-b0be-43cb-b58b-35579b037983",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = ff(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "28e53327-5a90-440d-92a9-556965b06b69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 16])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a65092f-c244-4fab-8445-4fefa1e3e54c",
   "metadata": {},
   "source": [
    "\n",
    "<center>\n",
    "    <img src=\"images/attention_layer.jpg\" width=\"700\">\n",
    "</center>\n",
    "\n",
    "# Basic Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5f2ef518-7a44-413c-883c-faacb8e297a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer_basic(nn.Module):\n",
    "    def __init__(self, d_model, heads, dropout_pct = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.norm_1 = Norm(d_model)\n",
    "        self.norm_2 = Norm(d_model)\n",
    "        self.attn = MultiHeadAttention(heads, d_model, dropout=dropout_pct)\n",
    "        \n",
    "        self.ff = FeedForward(d_model, dropout_pct = dropout_pct)\n",
    "        self.dropout_1 = nn.Dropout(dropout_pct)\n",
    "        self.dropout_2 = nn.Dropout(dropout_pct)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        # q = k = v = x\n",
    "        x = x + self.dropout_1(self.attn(x,x,x,mask))\n",
    "        x = self.norm_1(x)\n",
    "        \n",
    "        x = x + self.dropout_2(self.ff(x))\n",
    "        x = self.norm_2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f267ead-212d-4335-8e7e-ff1498d321d6",
   "metadata": {},
   "source": [
    "**Note:** The above implementation creates the `forward` function based on the above encoder image. Which is slightly different from this blog implemention [link](https://github.com/SamLynnEvans/Transformer/blob/master/Layers.py), where their `add` and `norm` operation order is slightly different\n",
    "\n",
    "```python\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, dropout=0.1):\n",
    "        pass\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        x2 = self.norm_1(x)\n",
    "        x = x + self.dropout_1(self.attn(x2,x2,x2,mask))\n",
    "        x2 = self.norm_2(x)\n",
    "        x = x + self.dropout_2(self.ff(x2))\n",
    "        return x\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6346f7-6647-41e7-b450-92edbbd387a6",
   "metadata": {},
   "source": [
    "## Test Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "73ad5cfe-2074-48a2-837f-9d828ffbca16",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 16\n",
    "heads = 4\n",
    "bs = 2\n",
    "seq_len = 10\n",
    "\n",
    "el = EncoderLayer_basic(d_model, heads)\n",
    "\n",
    "x = torch.rand(size=(bs,seq_len, d_model))\n",
    "\n",
    "output = el(x,mask=None)\n",
    "output.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f233930-4874-431a-9d77-397b595139b2",
   "metadata": {},
   "source": [
    "# Basic Decoder Layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "92a7b55d-74e1-4716-aedf-ebd5599bfab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer_basic(nn.Module):\n",
    "    def __init__(self, d_model, heads, dropout_pct = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.norm_1 = Norm(d_model)\n",
    "        self.norm_2 = Norm(d_model)\n",
    "        self.norm_3 = Norm(d_model)\n",
    "        \n",
    "        self.attn_decoder = MultiHeadAttention(heads, d_model)\n",
    "        self.attn_encoder = MultiHeadAttention(heads, d_model)\n",
    "        \n",
    "        self.ff = FeedForward(d_model)\n",
    "        \n",
    "        self.dropout_1 = nn.Dropout(dropout_pct)\n",
    "        self.dropout_2 = nn.Dropout(dropout_pct)\n",
    "        self.dropout_3 = nn.Dropout(dropout_pct)\n",
    "        \n",
    "    def forward(self, x, encoder_output, src_mask, trg_mask):\n",
    "        \n",
    "        \"\"\"\n",
    "        x: this x comes from the target language\n",
    "        \"\"\"\n",
    "        \n",
    "        q = k = v = x\n",
    "        x = x + self.dropout_1(self.attn_decoder(q,k,v,trg_mask))\n",
    "        x = self.norm_1(x)\n",
    "        \n",
    "        k_enc = v_enc = encoder_output\n",
    "        x = x + self.dropout_2(self.attn_encoder(x, k_enc, v_enc, src_mask))\n",
    "        x = self.norm_2(x)\n",
    "        \n",
    "        x = x + self.dropout_3(self.ff(x))\n",
    "        x = self.norm_3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101cd33f-13ac-49c2-8ee9-83bbd6a6279d",
   "metadata": {},
   "source": [
    "## Test Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c5e5de9c-fd29-422c-b724-e5e75baf9fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 16\n",
    "heads = 4\n",
    "bs = 2\n",
    "seq_len = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c8e2528f-a78f-40f2-a9a9-4d4b209f00f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "el = EncoderLayer(d_model, heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5252c377-41c9-402b-9fe9-e9200ee37364",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_src = torch.rand(size=(bs,seq_len, d_model))\n",
    "x_trg = torch.rand(size=(bs,seq_len, d_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c2dd2701-6fb1-4de2-b18a-e88444d7262f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 16])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_output = el(x_src,mask=None)\n",
    "enc_output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e73e8448-7b8a-4d98-86a5-eaae477a1657",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DecoderLayer_basic(d_model, heads)\n",
    "\n",
    "dec_output = dl(x_trg, enc_output, src_mask=None, trg_mask=None)\n",
    "\n",
    "dec_output.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd95ccc-b2c4-4394-928e-50133ec69ec8",
   "metadata": {},
   "source": [
    "# Add-Norm Module\n",
    "\n",
    "The above implementation of `EncoderLayer_basic()`, `DecoderLayer_basic()` ic correct. However we can create an `Add-Norm` module to make the code more modular and get rid of repeatitive code-block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "727a06b9-8688-4f88-9f4a-2bb5e6f7296b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNorm(nn.Module):\n",
    "    def __init__(self, d_model, dropout_pct = 0.1):\n",
    "        super().__init__()\n",
    "        self.norm = Norm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout_pct)\n",
    "        \n",
    "    def forward(self, x, attn_output):\n",
    "        # add\n",
    "        x = x + self.dropout(attn_output)\n",
    "        \n",
    "        # normalize\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ddad0b-03f9-4597-8f96-4b2e52f8224f",
   "metadata": {},
   "source": [
    "# Improved Encode Layer with AddNorm module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d5500724-570b-42e0-ab0b-73d020a199c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, dropout_pct = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attn = MultiHeadAttention(heads, d_model, dropout=dropout_pct)\n",
    "        self.ff = FeedForward(d_model, dropout_pct = dropout_pct)\n",
    "        self.add_norm_1 = AddNorm(d_model, dropout_pct)\n",
    "        self.add_norm_2 = AddNorm(d_model, dropout_pct)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        # q = k = v = x\n",
    "        x = self.add_norm_1(x,self.attn(x,x,x,mask))\n",
    "        x = self.add_norm_2(x, self.ff(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a0a767-f921-4a86-a443-f58541cc458f",
   "metadata": {},
   "source": [
    "## Test Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "935a5dab-0c9d-46ae-aa5b-2e9c30e55f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 16])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_model = 16\n",
    "heads = 4\n",
    "bs = 2\n",
    "seq_len = 10\n",
    "\n",
    "el_1 = EncoderLayer(d_model, heads)\n",
    "\n",
    "x = torch.rand(size=(bs,seq_len, d_model))\n",
    "\n",
    "output = el_1(x,mask=None)\n",
    "output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8a3e9806-80f4-4c76-ad7f-9c2cde96fd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, dropout_pct = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attn_decoder = MultiHeadAttention(heads, d_model)\n",
    "        self.attn_encoder = MultiHeadAttention(heads, d_model)\n",
    "        \n",
    "        self.ff = FeedForward(d_model)\n",
    "        \n",
    "        self.add_norm_1 = AddNorm(d_model, dropout_pct)\n",
    "        self.add_norm_2 = AddNorm(d_model, dropout_pct)\n",
    "        self.add_norm_3 = AddNorm(d_model, dropout_pct)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x, encoder_output, src_mask, trg_mask):\n",
    "        \n",
    "        \"\"\"\n",
    "        x: this x comes from the target language\n",
    "        \"\"\"\n",
    "        \n",
    "        q = k = v = x\n",
    "        x = self.add_norm_1(x ,self.attn_decoder(q,k,v,trg_mask))\n",
    "        \n",
    "        k_enc = v_enc = encoder_output\n",
    "        x = self.add_norm_2(x, self.attn_encoder(x, k_enc, v_enc, src_mask))\n",
    "        \n",
    "        x = self.add_norm_3(x, self.ff(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e16879-1573-410e-a135-06c5c246263f",
   "metadata": {},
   "source": [
    "## Test Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1550580d-4304-45b4-b8fe-1b4f553c8bcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 16])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_model = 16\n",
    "heads = 4\n",
    "bs = 2\n",
    "seq_len = 10\n",
    "\n",
    "el = EncoderLayer(d_model, heads)\n",
    "x_src = torch.rand(size=(bs,seq_len, d_model))\n",
    "enc_output = el(x_src,mask=None)\n",
    "\n",
    "\n",
    "dl = DecoderLayer(d_model, heads)\n",
    "x_trg = torch.rand(size=(bs,seq_len, d_model))\n",
    "dec_output = dl(x_trg, enc_output, src_mask=None, trg_mask=None)\n",
    "\n",
    "dec_output.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50ae044-57b3-4b58-ad0b-219bb0b5d39d",
   "metadata": {},
   "source": [
    "# Replication of EncoderLayer and DecoderLayer\n",
    "\n",
    "One last Variable: If you look at the diagram closely you can see a `Nx` next to the encoder and decoder architectures. In reality, the encoder and decoder in the diagram above represent one layer of an encoder and one of the decoder. `N` is the variable for the number of layers there will be. Eg. if `N=6`, the data goes through six encoder layers (with the architecture seen above), then these outputs are passed to the decoder which also consists of six repeating decoder layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4e892065-32a2-4081-ad91-6032113aeb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c9058c1e-bee5-4e66-b947-b1fe388e9c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f4a44118-6c6a-4178-983a-7d4244da5f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_encoder_layer, heads, dropout):\n",
    "        super().__init__()\n",
    "        self.n_encoder_layer = n_encoder_layer\n",
    "        self.embed = Embedder(vocab_size, d_model)\n",
    "        self.pe = PositionalEmbedding(d_model)\n",
    "        enc_layer = EncoderLayer(d_model, heads, dropout)\n",
    "        # stack enc_layers\n",
    "        self.enc_layers = get_clones(enc_layer, n_encoder_layer)\n",
    "        \n",
    "    def forward(self, src_token, mask):\n",
    "        x = self.embed(src_token)\n",
    "        x = self.pe(x)\n",
    "        \n",
    "        for i in range(self.n_encoder_layer):\n",
    "            x = self.enc_layers[i](x, mask)\n",
    "        \n",
    "        # the EncoderLayer return normalized x\n",
    "        # so no need to pass through Norm() again\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "248466cc-2e24-4c6c-b0b9-ba9910c01413",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 16\n",
    "heads = 4\n",
    "bs = 2\n",
    "seq_len = 10\n",
    "vocab_size = 2000\n",
    "\n",
    "enc = Encoder(vocab_size, d_model, \n",
    "              n_encoder_layer=6, heads=heads, dropout=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "3bb967fd-be95-4bf5-b6dd-d033845ad6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_src = torch.tensor([1,2,3,4,5, 6,7, 8, 9, 10])\n",
    "enc_output = enc(x_src,mask=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d90f1190-c932-4279-bbef-f485278ad5f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 10, 16])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d9da14-9628-4e81-9235-3cb944256d0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
