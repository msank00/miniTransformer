{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3c823b1-306b-4906-be34-8cb8259e044b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dbc4ad7-9e99-4007-8a75-10dc82eb22a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064361cf-bfba-43bf-a8bb-3eaff80c015e",
   "metadata": {},
   "source": [
    "# Embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ece1155a-7757-4dbb-821c-f84501a19bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder(nn.Module):\n",
    "    def __init__(self, vocab_size:int, emb_dim:int):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim \n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim)\n",
    "\n",
    "    def forward(self, x: torch.tensor):\n",
    "        return self.embed(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1f847a-d0c4-4944-8898-9ea00c9fe419",
   "metadata": {},
   "source": [
    "# Why use `register_buffer`\n",
    "\n",
    "- Ans [link](https://discuss.pytorch.org/t/what-is-the-difference-between-register-buffer-and-register-parameter-of-nn-module/32723/11)\n",
    "\n",
    "An example where I find this distinction difficult is in \n",
    "the context of fixed positional encodings in the Transformer \n",
    "model. Typically I see implementations where the fixed positional \n",
    "encodings are registered as buffers but I’d consider these tensors \n",
    "as non-learnable parameters (that should show up in the list of \n",
    "model parameters), especially when comparing between methods \n",
    "that don’t rely on such injection of fixed tensors.\n",
    "\n",
    "So in general:\n",
    "- buffers = `fixed tensors / non-learnable parameters / stuff that does not require gradient`\n",
    "- parameters = `learnable parameters, requires gradient`\n",
    "\n",
    "![image](https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/ptrblck/90/1823_2.png)\n",
    "Piotr Bialecki\n",
    "\n",
    "If you have parameters in your model, which should be saved and restored in the state_dict, but not trained by the optimizer, you should register them as buffers. Buffers won’t be returned in model.parameters(), so that the optimizer won’t have a change to update them.\n",
    "\n",
    "Both approaches work the same regarding training etc.\n",
    "There are some differences in the function calls however. Using register_parameter you have to pass the name as a string, which can make the creation of a range of parameters convenient. Besides that I think it’s just coding style which one you prefer.\n",
    "\n",
    "If your `self.some_params` are `nn.Parameter` objects, then you don’t have to worry about this. If they’re tensors, then they won’t be in the `state_dict` (unless registered as buffer).\n",
    "\n",
    "> simple `torch.tensor` will not be available under `state_dict`\n",
    "\n",
    "one reason to register the tensor as a buffer is to be able to serialize the model and restore all internal states.\n",
    "Another one is that all buffers and parameters will be pushed to the device, if called on the parent model:\n",
    "\n",
    "```python\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.my_tensor = torch.randn(1)\n",
    "        self.register_buffer('my_buffer', torch.randn(1))\n",
    "        self.my_param = nn.Parameter(torch.randn(1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "            return x\n",
    "\n",
    "model = MyModel()\n",
    "print(model.my_tensor)\n",
    "> tensor([0.9329])\n",
    "print(model.state_dict())\n",
    "> OrderedDict([('my_param', tensor([-0.2471])), ('my_buffer', tensor([1.2112]))])\n",
    "\n",
    "model.cuda()\n",
    "print(model.my_tensor)\n",
    "> tensor([0.9329])\n",
    "print(model.state_dict())\n",
    "> OrderedDict([('my_param', tensor([-0.2471], device='cuda:0')), ('my_buffer', tensor([1.2112], device='cuda:0'))])\n",
    "```\n",
    "\n",
    "As you can see, model.my_tensor is still on the CPU, where is was created, while all parameters and buffers were pushed to the GPU after calling `model.cuda()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200974c3-2498-4e28-aa17-29a3d6826397",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"https://miro.medium.com/max/566/1*B-VR6R5vJl3Y7jbMNf5Fpw.png\" width=\"400\">\n",
    "</center>\n",
    "\n",
    "# Make embedding relatively larger by scaling the values. WHY?\n",
    "\n",
    "The reason we increase the embedding values before \n",
    "addition is to make the positional encoding relatively \n",
    "smaller. This means the original meaning in the embedding \n",
    "vector won’t be lost when we add them together\n",
    "\n",
    "```python\n",
    "x = x*math.sqrt(self.emb_dim)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "31365422-2cda-43d6-947f-8554939ccad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, emb_dim:int, max_seq_len:int = 200, dropout_pct:float = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.dropout = nn.Dropout(dropout_pct)\n",
    "\n",
    "        # create constant 'pe' matrix with values dependent on \n",
    "        # word position 'pos' and embedding position 'i'\n",
    "        pe = torch.zeros(max_seq_len, emb_dim)\n",
    "\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, emb_dim, 2):\n",
    "                pe[pos,i] = math.sin(pos/(1000**((2*i)/emb_dim)))\n",
    "                pe[pos,i+1] = math.sin(pos/(1000**((2*(i+1))/emb_dim)))\n",
    "\n",
    "        # print(pe.size())\n",
    "        # pe = pe.unsqueeze(0)\n",
    "        # print(pe.size())\n",
    "\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # scale values\n",
    "        x = x*math.sqrt(self.emb_dim)  \n",
    "\n",
    "        seq_len = x.size(0)\n",
    "        \n",
    "        # add constant positional embedding to the word embedding\n",
    "        pe = Variable(self.pe[:seq_len,:], requires_grad=False)\n",
    "        \n",
    "        if x.is_cuda:\n",
    "            pe.cuda()\n",
    "        \n",
    "        x = x + pe\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379c04b9-bcdf-4eb2-8b0a-b89bc83332d7",
   "metadata": {},
   "source": [
    "## Test module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "fbab3d5a-002d-4347-b45c-081a2358b93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 20000\n",
    "EMB_DIM = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "498d138b-f422-4578-8fa3-4fa02e3034e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = Embedder(VOCAB_SIZE, EMB_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "767a1dea-4fce-4307-a93b-c0e016b78a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = torch.tensor([1,2,3,4,5]) # torch.randint(3, 5, (3,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "b73df2a1-9e51-41e7-92e4-a25a78974a3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "f09c443d-29ce-40ab-a2e4-f6243cb3ead9",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = e(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "dd525d96-7abf-48ba-abf1-6b03929a027a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 512])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "adbe5873-f596-47e2-9a6e-ff9e50f8420b",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = PositionalEmbedding(emb_dim=EMB_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "3a1e93fd-04ac-451c-9964-4e58b6491e03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 512])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.pe.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "c62a48cb-004a-4e5e-ac9b-b82a5718752f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 512])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.pe.squeeze().size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "ca3827bb-906c-471e-8c8d-c01992587f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_emb = p(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "3b14d4fc-dea2-43b8-a957-d6989c999e10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 512])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_emb.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42cacf2-5830-42ff-9f92-3ba0e3c3d383",
   "metadata": {},
   "source": [
    "# Batch Normalization\n",
    "\n",
    "Normalisation is highly important in deep neural networks. It prevents the range of values in the layers changing too much, meaning the model trains faster and has better ability to generalise.\n",
    "\n",
    "<center>\n",
    "    <img src=\"https://miro.medium.com/max/511/1*4w3sQ14caDRkrQsAeK5Flw.png\" width=\"400\">\n",
    "</center>\n",
    "\n",
    "We will be normalising our results between each layer in the encoder/decoder, so before building our model let’s define that function:\n",
    "\n",
    "- [blog](https://kharshit.github.io/blog/2018/12/28/why-batch-normalization)\n",
    "\n",
    "<center>\n",
    "    <img src=\"https://kharshit.github.io/img/batch_normalization.png\" width=\"400\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "62b42188-7c89-4370-a146-019e8646eaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Norm(nn.Module):\n",
    "    def __init__(self, d_model, eps = 1e-6):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.size = d_model\n",
    "        \n",
    "        # create two learnable parameters to calibrate normalisation\n",
    "        self.alpha = nn.Parameter(torch.ones(self.size))\n",
    "        self.bias = nn.Parameter(torch.zeros(self.size))\n",
    "        \n",
    "        self.eps = eps\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x_mean = x.mean(dim=-1, keepdim=True)\n",
    "        x_variance = x.std(dim=-1, keepdim=True) \n",
    "        \n",
    "        normalized_x = (x - x_mean) / (x_variance + self.eps)\n",
    "        \n",
    "        # scale and shift\n",
    "        y = self.alpha * normalized_x + self.bias\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5f9182-0a0b-4b08-bef2-912ef6a41975",
   "metadata": {},
   "source": [
    "## Test module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "393b7769-337a-420b-b788-49633a359d3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.6995, 0.8797, 0.3724, 0.2287, 0.4503],\n",
       "         [0.9911, 0.6278, 0.0657, 0.3674, 0.2664],\n",
       "         [0.9776, 0.4216, 0.2456, 0.8553, 0.1205],\n",
       "         [0.3701, 0.1670, 0.7497, 0.4190, 0.3809],\n",
       "         [0.2739, 0.2522, 0.5973, 0.6230, 0.7718],\n",
       "         [0.8050, 0.0362, 0.4502, 0.5093, 0.9923],\n",
       "         [0.9708, 0.5319, 0.8902, 0.8937, 0.0268],\n",
       "         [0.6310, 0.3133, 0.3671, 0.5056, 0.5416],\n",
       "         [0.9670, 0.6497, 0.8942, 0.6173, 0.3297],\n",
       "         [0.8384, 0.7972, 0.8147, 0.2408, 0.3032]],\n",
       "\n",
       "        [[0.1791, 0.7929, 0.8694, 0.4868, 0.7919],\n",
       "         [0.2966, 0.0549, 0.5732, 0.8265, 0.9248],\n",
       "         [0.5742, 0.8801, 0.1025, 0.9999, 0.7294],\n",
       "         [0.4161, 0.3759, 0.0544, 0.9366, 0.2160],\n",
       "         [0.5113, 0.1617, 0.3394, 0.9271, 0.2979],\n",
       "         [0.5145, 0.8691, 0.2940, 0.0353, 0.1214],\n",
       "         [0.4028, 0.7736, 0.6155, 0.2101, 0.9731],\n",
       "         [0.8073, 0.2950, 0.9592, 0.8006, 0.8004],\n",
       "         [0.7588, 0.8528, 0.7546, 0.1169, 0.4026],\n",
       "         [0.1103, 0.0015, 0.7686, 0.0226, 0.9467]]])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_model = 5\n",
    "bs = 2\n",
    "seq_len = 10\n",
    "x = torch.rand(size=(bs,seq_len, d_model))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "e7e738aa-2165-4f8e-8b38-af80adb10ac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 5])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "0a5bc488-5b0a-4261-84a7-faea341686c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = Norm(d_model=d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "00065cd0-bbc9-45bf-934d-07e5cf0c2461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.6639,  1.3536, -0.5886, -1.1386, -0.2903],\n",
       "         [ 1.4746,  0.4588, -1.1127, -0.2691, -0.5517],\n",
       "         [ 1.2053, -0.2724, -0.7403,  0.8801, -1.0726],\n",
       "         [-0.2249, -1.1910,  1.5814,  0.0078, -0.1732],\n",
       "         [-1.0005, -1.0948,  0.4079,  0.5196,  1.1678],\n",
       "         [ 0.6731, -1.4272, -0.2962, -0.1347,  1.1849],\n",
       "         [ 0.7816, -0.3317,  0.5771,  0.5862, -1.6132],\n",
       "         [ 1.2266, -1.2201, -0.8055,  0.2608,  0.5382],\n",
       "         [ 1.0904, -0.1659,  0.8023, -0.2940, -1.4329],\n",
       "         [ 0.7996,  0.6621,  0.7206, -1.1953, -0.9870]],\n",
       "\n",
       "        [[-1.5405,  0.5848,  0.8495, -0.4750,  0.5812],\n",
       "         [-0.6582, -1.3249,  0.1048,  0.8035,  1.0747],\n",
       "         [-0.2379,  0.6388, -1.5899,  0.9822,  0.2068],\n",
       "         [ 0.0491, -0.0719, -1.0391,  1.6147, -0.5528],\n",
       "         [ 0.2159, -0.9662, -0.3655,  1.6218, -0.5059],\n",
       "         [ 0.4406,  1.4981, -0.2175, -0.9890, -0.7323],\n",
       "         [-0.6405,  0.5949,  0.0683, -1.2824,  1.2596],\n",
       "         [ 0.2946, -1.7238,  0.8934,  0.2685,  0.2674],\n",
       "         [ 0.5873,  0.8911,  0.5737, -1.4877, -0.5643],\n",
       "         [-0.5751, -0.8160,  0.8829, -0.7693,  1.2775]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81828f13-b3c5-4cea-9db3-3e00f1311722",
   "metadata": {},
   "source": [
    "# Attention\n",
    "\n",
    "<center>\n",
    "    <img src=\"https://miro.medium.com/max/445/1*evdACdTOBT5j1g1nXialBg.png\" width=\"400\">\n",
    "</center>\n",
    "\n",
    "\n",
    "<center>\n",
    "    <img src=\"https://miro.medium.com/max/140/1*15E9qKg9bKnWdSRWCyY2iA.png\" width=\"100\">\n",
    "</center>\n",
    "\n",
    "- Initially we must multiply $Q$ by the transpose of $K$. This is then `scaled` by dividing the output by the square root of $d_k$.\n",
    "- A step that’s not shown in the equation is the `masking operation`. Before we perform `Softmax`, we apply our mask and hence reduce values where the input is padding (or in the decoder, also where the input is ahead of the current word).\n",
    "\n",
    "Another step not shown is `dropout`, which we will apply after `Softmax`.\n",
    "\n",
    "Finally, the last step is doing a `dot` product between the result so far and $V$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "c297cf51-86df-4ef0-bc85-c2e586c3407d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(q, k, v, d_k, mask=None, dropout=None):\n",
    "    \n",
    "    scores = torch.matmul(q, k.transpose(-2, -1)) /  math.sqrt(d_k)\n",
    "    \n",
    "    if mask is not None:\n",
    "        mask = mask.unsqueeze(1)\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    \n",
    "    scores = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    if dropout is not None:\n",
    "        scores = dropout(scores)\n",
    "        \n",
    "    output = torch.matmul(scores, v)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702ed8c6-1039-4511-b3ae-01fd7ff0f7fe",
   "metadata": {},
   "source": [
    "# Multi-Headed Attention\n",
    "\n",
    "Once we have our embedded values (with positional encodings) and our masks, we can start building the layers of our model.\n",
    "\n",
    "Here is an overview of the multi-headed attention layer:\n",
    "\n",
    "\n",
    "<center>\n",
    "    <img src=\"https://miro.medium.com/max/523/1*1tsRtfaY9z6HxmERYhw8XQ.png\" width=\"300\">\n",
    "</center>\n",
    "\n",
    "- $V$, $K$ and $Q$ stand for `key`, `value` and `query`. These are terms used in attention functions\n",
    "\n",
    "- In the case of the **Encoder**, $V, K$ and $G$ will simply be **identical copies** of the `emb_vector + pos_encoding`. \n",
    "- They will have the dimensions `Batch_size * seq_len * d_model`\n",
    "\n",
    "<center>\n",
    "    <img src=\"images/tensor_dimension.png\" width=\"400\">\n",
    "</center>\n",
    "\n",
    "- Drawing tool [Excalidraw](https://excalidraw.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "5444bcb0-2cb0-482f-a4eb-91ab675e8c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads, d_model, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // heads\n",
    "        self.h = heads\n",
    "        \n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \n",
    "        bs = q.size(0)\n",
    "        \n",
    "        # perform linear operation and split into N heads\n",
    "        k = self.k_linear(k).view(bs, -1, self.h, self.d_k)\n",
    "        q = self.q_linear(q).view(bs, -1, self.h, self.d_k)\n",
    "        v = self.v_linear(v).view(bs, -1, self.h, self.d_k)\n",
    "        \n",
    "        # transpose to get dimensions bs * N * sl * d_model\n",
    "        k = k.transpose(1,2)\n",
    "        q = q.transpose(1,2)\n",
    "        v = v.transpose(1,2)\n",
    "        \n",
    "\n",
    "        # calculate attention using function we will define next\n",
    "        scores = attention(q, k, v, self.d_k, mask, self.dropout)\n",
    "        # concatenate heads and put through final linear layer\n",
    "        concat = scores.transpose(1,2).contiguous()\\\n",
    "        .view(bs, -1, self.d_model)\n",
    "        output = self.out(concat)\n",
    "    \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451e0060-f7cf-4f0e-9450-05695776dd30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
